---
layout: post
title:  "Baird's counterexample"
date:   2020-08-16 15:10:15 +1000
categories: jekyll update
---
This is to produce Figure 11.2, Figure 11.5, Figure 11.6 in RL book.

<div style="text-align:center"><img src="/files/Chapter11/Bairds_CE/BC_p1.PNG" alt="drawing" width="500"/></div>
*(source: Reinforcement Learning: An Introduction, 2nd Edition, by Richard S. Sutton and Andrew G. Barto)*


## **Problem description**

The problem is well described in above figure. Let's summarize it.

**State**: [state]. '0' represents the lower state, (from 1 to 6) represents the six upper state. \
**Action**: 0 or 1. 0 represents dashed action, leading to any of the six upper states randomly. '1' represents the solid action, leading to the lower state deterministically. \
**Reward**: always zero

The state value is approximated using linear function approximation. The feature vector corresponding to that assigned in above figure is:

[0,0,0,0,0,0,1,2] for state 0 \
[2,0,0,0,0,0,0,1] for state 1 \
[0,2,0,0,0,0,0,1] for state 2 \
[0,0,2,0,0,0,0,1] for state 3 \
[0,0,0,2,0,0,0,1] for state 4 \
[0,0,0,0,2,0,0,1] for state 5 \
[0,0,0,0,0,2,0,1] for state 6


## **Algorithm**

The book didn't give any pseudo code for all these figures. Therefore presented below are pseudo code I used to generated these figures.

Semi-gradient Off-policy TD (for Figure 11.2 left):

<div style="text-align:center"><img src="/files/Chapter11/Bairds_CE/BC_p2.PNG" alt="drawing" width="600"/></div>

Semi-gradient DP (for Figure 11.2 right):

<div style="text-align:center"><img src="/files/Chapter11/Bairds_CE/BC_p3.PNG" alt="drawing" width="600"/></div>

TD(0) with gradient correction (TDC) (for Figure 11.5 left):

<div style="text-align:center"><img src="/files/Chapter11/Bairds_CE/BC_p4.PNG" alt="drawing" width="600"/></div>

Expected TDC (for Figure 11.5 right):

<div style="text-align:center"><img src="/files/Chapter11/Bairds_CE/BC_p5.PNG" alt="drawing" width="600"/></div>

Expected Emphatic-TD (for Figure 11.6):

<div style="text-align:center"><img src="/files/Chapter11/Bairds_CE/BC_p6.PNG" alt="drawing" width="600"/></div>

The formula for calculating Projection Bellman Error given in equation(11.25) in the book:

$$\overline{PBE}(\mathbf{w}) = ||\Pi \overline{\delta}_{\mathbf{w}}||^2_{\mu} = (\Pi \overline{\delta}_{\mathbf{w}})^{\top} \mathbf{D} (\Pi \overline{\delta}_{\mathbf{w}})$$

where the Bellman error is:

$$\overline{\delta}_{\mathbf{w}}(s) = \left( \sum_a \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r | s,a) [r + \gamma v_{mathbf{w}}(s^{\prime})] \right) - v_{\mathbf{w}}(s)$$

and $$\overline{\delta}_{\mathbf{w}}$$ is a vector encapsulating $$\overline{\delta}_{\mathbf{w}}(s)$$ for all the all states as its elements.

Note that we'd better do the calculation for PBE following the order as shown in above formula, rather than that in equation (11.26). Using (11.26) for PBE is not appropriate in terms of computational accuracy as the sequence of calculation may cause some rounding error. In practice, the PBE curve generated by (11.26) is full of oscillation and not smooth.

The formula for calculating Mean Squared Value Error ($$\overline{VE}$$) is given in Chapter 9:

$$\overline{VE}(\mathbf{w}) = \sum \limits_{s \in S} \mu(s) [v_{\pi}(s) - \hat{v}(s,\mathbf{w})]^2$$

where in this case $$\mu(s) = 1/7$$ for all states, since they are equally distributed.



## **Implementation**

All parameters for the following figures are clearly stated in the book.

We get Figure 11.2 as follows:

<img src="/files/Chapter11/Bairds_CE/Figure11_2_left.svg" alt="drawing" width="350"/>
<img src="/files/Chapter11/Bairds_CE/Figure11_2_right.svg" alt="drawing" width="350"/>

We get Figure 11.5:

<img src="/files/Chapter11/Bairds_CE/Figure11_5_left.svg" alt="drawing" width="350"/>
<img src="/files/Chapter11/Bairds_CE/Figure11_5_right.svg" alt="drawing" width="350"/>

We get Figure 11.6:

<div style="text-align:center"><img src="/files/Chapter11/Bairds_CE/Figure11_6.svg" alt="drawing" width="450"/></div>

This figure is a little different than that in the book. It seems in our figure it converges faster. It is probably due to some details (synchronized or asynchronized updates of emphasis M).




## **Code Usage**

Download the code [Bairds_Counterexample](https://github.com/liCCcccs/Reinforcement-Learning-Book-Reproduce/tree/master/Chapter11/Bairds_Counterexample).

To produce the two figures in Figure 11.2:
{% highlight Bash %}
python3 Get_data_TD0.py
python3 Get_data_DP.py {% endhighlight %}

To produce the two figures in Figure 11.5:
{% highlight Bash %}
python3 Get_data_TDC.py
python3 Get_data_TDC_Expected.py {% endhighlight %}

To produce Figure 11.6:
{% highlight Bash %}
python3 Get_data_Emphatic_TD_Expected.py {% endhighlight %}



**The End**
