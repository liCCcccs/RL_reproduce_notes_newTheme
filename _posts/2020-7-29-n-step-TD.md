---
title:  "n-step TD method on the Random Walk"
date:   2020-07-29 16:10:15 +1000
categories: jekyll update
permalink: /RL_notes/n-step-TD/
---
<div style="text-align:center"><img src="/files/Chapter7/TDn/RW.PNG" alt="drawing" width="500"/></div>
*(source: Reinforcement Learning: An Introduction, 2nd Edition, by Richard S. Sutton and Andrew G. Barto)*

This is Example 7.1 in RL book.

## **Problem discription**

The purpose is to evaluate the performance of n-step TD methods with difference choice of step size $$\alpha$$ and parameter $$n$$, on the Random Walk problem.

There are in total 19 states, excluding two terminal states on the left and right. The reward is -1 if terminates on the left, 1 if terminates on the right, and 0 on other transitions. The policy is to go left and right with equal probability. The true values for the 19 states are $$-0.9, -0.8, -0.7, ......, 0.7,0.8,0.9$$ from left to right respectively.


## **Implementation**

The algorithm for plotting Figure 7.2 is n-step TD method, whose pseudo code is given below. and the resulting plot are as follows:

<div style="text-align:center"><img src="/files/Chapter7/TDn/TDn_p1.PNG" alt="drawing" width="600"/></div>
*(source: Reinforcement Learning: An Introduction, 2nd Edition, by Richard S. Sutton and Andrew G. Barto)*

Following exactly the n-step TD estimation algorithm, we can plot Figure 7.2. One thing to notice in terms of of plotting the figure is that, the interval of value of $$\alpha$$ is not constant. In addition, it has been tested that with only 100 repetitions the curves are not smooth enough, thus to generate the figure we averaged over 1000 repetition. We get Figure 7.2 as follows.

<div style="text-align:center"><img src="/files/Chapter7/TDn/TDn_title.svg" alt="drawing" width="600"/></div>

Also, you can simulate one run of n-step TD method, and produce plots similar to the left figure in Example 6.2 in the book:
<div style="text-align:center"><img src="/files/Chapter7/TDn/Single_TDn.svg" alt="drawing" width="500"/></div>



## **Code Usage**

Download the code [n_step_Random_Walk](https://github.com/liCCcccs/Reinforcement-Learning-Book-Reproduce/tree/master/Chapter7/n_step_Random_Walk).

To simulate one run of Random Walk:
{% highlight Bash %}
python3 n_step_TD.py {% endhighlight %}
You can change the parameters in the main function to customize your plot.

To generate Figure 7.2 in the book:

{% highlight Bash %}
python3 Get_TDn_data.py -n 1
python3 Get_TDn_data.py -n 2
python3 Get_TDn_data.py -n 4
python3 Get_TDn_data.py -n 8
python3 Get_TDn_data.py -n 16
python3 Get_TDn_data.py -n 32
python3 Get_TDn_data.py -n 64
python3 Get_TDn_data.py -n 128
python3 Get_TDn_data.py -n 256
python3 Get_TDn_data.py -n 512
python3 plot_TDn.py {% endhighlight %}

Every simulation takes about 1 minute. I separate them in order to let the computer take a breath between simulations.







**The End**
